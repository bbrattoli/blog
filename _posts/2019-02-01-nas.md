---
layout: post
title: Neural Architecture Search
subtitle: past, present and future
tags: [NAS, MetaLearning]
feature-img: assets/img/sample_feature_img.png
hide: false
---
Summary into the past, present and future of Neural Architecture Search (NAS). 

## What is NAS?
Neural Architecture search (as the name suggests) are machine learning algorithms for automatic search of neural network architecture. 
Anyone who had contact with deep learning knows that finding the hyperparameters is very costly and takes most of the computational effort. The cost becames huge when you want to optimize the network architecture to the specific task, in particular defining the number of layers, the activation per layer, the number of filters per convolution, the skip connections, etc... Typically researchers (like me) do not care about the optimizing the architecture, since we are constraint by using the same architecture we are comparing our method with. However, optimizing the architecture for a specific task is crucial for industry, where deep learning engineers spend most of their time on the problem.

Things started to change in 2017 when a Google Residence student succesfully implemented an LSTM which creates the architecture layer by layer, in order to optimize the validation accuracy on CIFAR-10 using roughly 800 gpus for several weeks. At the time this created a lot of hype on the topic, however not many groups had that kind of computational power. In roughly two years the field evolved until it is now possible to outperform the initial success of Google using a single gpu for half a day. This post is intend to summarize the milestone from the first paper until today.

**Disclaimer**: I cannot cover all papers on the topic, thus I will focus on few which I personally found of interest. If you feel that some majour work has been left out, please send me an email, I am always interested in learning new things.

## Past
Until recently Google had the monopoly on the field, by publishing a series of papers (all from the original V.le team) and implementing a commercial product called AutoML which optimizes the network architecture for the specific task of the client. Just to give you a feeling of how quickly NAS went from research to production, recently Waymo announced a [collaboration with GoogleAI](https://medium.com/waymo/automl-automating-the-design-of-machine-learning-models-for-autonomous-driving-141a5583ec2a) for using AutoML to improve the self-driving car system. 

![Waymo tweet on the collaboration with AutoML]({{ site.baseurl }}/assets/img/post_NAS/waymo_tweet.png)

In this section I will give some more insights into three main papers from the Google team.

### Zoph and V.le
As already mentioned in the introuduction, the first NAS model was implemented bye a student during his [Google Residency](https://ai.google/research/join-us/ai-residency/) program (which I personally think it gives an extra spin to the story). Their work was published at ICLR17 with the title [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578). 

The model is overall quite simple: a recurrent neural network (two-layer LSTM), called controller, select a specific parameter of the network architecture per each step. In particular, the controller defines: filter hight and width, stride height and width, anchor point (for allowing skip connections) and number of filters. The parameters are chosen layer-by-layer, which means that the first 6 steps of the controller define the first layer, step 7 to 12 define the second layer of the final architecture, and so on untile the desired depth of the network is reached.

The controller learn by training the produced architecture on the training set of CIFAR-10 and evaluating on the validation set. This last evaluation is used as feedback signal for the controller, basically says to the controller if it did a good job or not. Since we cannot differentiate the validation loss with the decision process of the controller, more generally speacking there is no direct connection between the controller decision and the final loss, it is not possible to train the controller end-to-end with SGD. This is where Reinforcment Learning comes into rescue. Without going too much into details (I don't want to make this post a technical report), the validation accuracy is used as a reward function: high reward = controller did good, low reward = controller did bad. The controller can then be updated using REINFORCE by maximizing the expected reward using SGD.

As you can imagine, training the controller for producing the final "optimal" architecture is extremely costly since it is needed to train each single architecture until it converges. The final result reaches comparable performances with hand-crafted architecures, however it does not scale to Imagenet having 1.2M images for training and 1000 classes. The next work will focus exactly on the scalability issue.

### NASNet
The [next work of Zoph](https://arxiv.org/abs/1707.07012) (who as you expect was immediatly hired by Google) focuses on scalability. They tackeled the problem by breaking the task in two parts: first find the best sub-network (called cell) that performs best on the validation set, then combine copies of the cell to make the architecture deeper. The key attribute for the success of the method is transferability, in fact there is no garantee that solving the task for the cell gives a good solution for the deeper network (concatenation of cells), even more when the network is optimized for a dataset/task and applied on another. Likely neural network have been proven many and many times to be great when it comes to transferability.

The final architecture, called NASNet (probably spending a good 30 seconds brain-storming on the name), achived incredible results, beating existing architecture in different tasks (image classification and object detection) while using much less parameters. However, a huge amount of computation is still needed to train the controller.

The same team also published an approach which solves the task using Evolutionary strategy instead of Reinforcemente Learning, producing [AmeobaNet](https://arxiv.org/abs/1802.01548). This network slightly outperforms the previous one, however it needs even more computational power.

## Present

To summarize, Google was able to solve the NAS problem and implement a commercial solution in a bit more than a year. However, only an handfull of groups had access to it. Thankfully, the Google group published a new paper at ICML18 called [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268) which, as the name suggests, focus on making the NAS problem **efficient**. Before diving into the boring stuff, let's take a look at how efficient this is in the following table

Model     | Venue    | GPUs | Days | Cifar10 error | Imagenet top1 error
--------- | -----    | ---- | ---- | ------------- | -------------------
Zoph&V.le | ICLR17   | 800  | 28   | 3.65          |                    
NASNet    | CVPR18   | 500  | 4    | 2.65          |               26.0 
AmoebaNet | ICML-W18 | 450  | 7    | 3.12          |               24.3 
**ENAS**  | ICML18   |   1  | 0.45 | 2.89          |                    

The results of ENAS were very impressive and opened the field to a bigger group of people who can now favorite from NAS.

But how did they manage to obtain a boost of 1000x in computational speed? The key idea is to reformulate the architecture search problem from a tree-search, where each decision opens up a totally new path, to a **direct acyclic graph structure (DAG)**. Each node of the graph receives the concatenated output of multiple other nodes and produce its own output by applying a node specific function. In this way it is possible to share parameters across selected architectures, which makes the architecture training much faster. Per each node, the controller decides the input nodes and the node function from a predefined set of functions, which also reduces the search space. A path from the first to the last node of the graph defines the architecture.

## Future
Finally, we arrive at my favorite part of the story. Because of the complexity the reseach in the area was monotona, but thanks to the latest development many more groups jumped on the race and very interesting work already came out. I selected two of the most recent works which will be publish at ICLR19. 

### GHN
Since the heavy computation comes from training each proposed network, why not totally get rid of it? This is what the amazing Uber team led by Raquel Urtsun achived in their [Graph HyperNetworks for Neural Architecture Search](https://arxiv.org/abs/1810.05749). Their contribution is an hypernetwork which, given a neural network architecture, regresses the best weights. Once the hypernetwork has learnt, the only thing left is to sample a lot of random architecture, evaluate them on the validation set and pick the best one.

The hypernetwork is trained end-to-end by sampling a random architecture, inferring the weights, compute the loss and backpropagate through the network up to the hypernetwork. Genius! 

### DARTS
Given the non-differentiable nature of the decision problem, Google always tried to get around the problem using Reinforcmente Learning or Evolutionary strategy. The Yang's team from CMU collaborated with DeepMind to directly tacle the problem by making it differentiable, since we know much better how to optimize differentiable problems. Thei work is described in the paper [DARTS: Differentiable Architecture Search
](https://arxiv.org/abs/1806.09055).

The idea is based on the DAG from the ENAS, but instead of selecting one function per node, they apply **all functions at all nodes**. Then a coefficient $$\alpha$$ is assigned to each function which acts as a soft-assignment. The network is trained end-to-end in two phases: first the alpha is frozen and the weights of the graph are updated on the train loss, then the weights are frozen and the alpha are learned using the validation loss. After convergence, the architecture is chose simply by $$ argmax(\alpha) $$ per each node. I refer to the paper for more details, the math looks scary but it is actually very simple and easy to read. The results are not as good as the last two methods because the search space is now very complex (given that there are two concurrent optimization), but as I said we are very good at solving differentiable problems, next than you know it this will be the leading approach.

Before concluding, here is a final comparioson of the described methods:

Model     | Venue    | GPUs | Days | Cifar10 error | Imagenet top1 error
--------- | -----    | ---- | ---- | ------------- | -------------------
Zoph&V.le | ICLR17   | 800  | 28   | 3.65          |                    
NASNet    | CVPR18   | 500  | 4    | 2.65          |               26.0 
AmoebaNet | ICML-W18 | 450  | 7    | 3.12          |               24.3 
ENAS      | ICML18   |   1  | 0.45 | 2.89          |                    
GHN       | ICLR19   |   1  | 0.85 | 2.84          |               27.0
DARTS     | ICLR19   |   1  | 4    | 2.83          |               26.9 



### What now?
In conclusion, this was the brief history of NAS, however we are still at the beginning. As mentioned before, expecially industry has a big need for this technology, so many and many more groups are joining the raise and, solved the computational limit of the approach, only creativity can stop us. Given that the attempt was published barely two years ago and today NAS can already beating years of human research in half-a-day of computation, I am very exited to see where we will in a year or so.


## references


